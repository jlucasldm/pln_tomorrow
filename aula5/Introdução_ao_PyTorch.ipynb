{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0asTvTxnBvyw"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'DataLoader' from 'torch.utils.data' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Baixar por\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'DataLoader' from 'torch.utils.data' (unknown location)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Baixar por\n",
        "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPZmaSOWFsXu",
        "outputId": "26f5e219-8d95-4143-9f4a-9a5aa91f666f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_RiBw8hGbf_"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU () ,\n",
        "        nn.Linear(512,512),\n",
        "        nn.ReLU (),\n",
        "        nn.Linear(512,10),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU ()\n",
        "    )\n",
        "    self.linear_tahn = nn.Sequential(\n",
        "       nn.Linear(512, 512),\n",
        "       nn.Relu()\n",
        "    )\n",
        "    self.conv = torch.nn.Conv1d(256, 1, 2)\n",
        "  def forward(self,x):\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        x = self.linear_relu(x)\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        x = self.linear_tahn(x)\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        x = x.reshape((256, 2))\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        logits = self.conv(x)\n",
        "        print(\"logits.shape: \", logits.shape)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knr6lyc8M00k",
        "outputId": "3f91c5a6-7d92-4d94-a8ca-2178462f00b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnqeHQeuOClZ",
        "outputId": "cf08ded1-80d9-4d18-878c-805a60ccfa48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([7])\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feito em sala\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu = nn.Sequential(\n",
        "        nn.Linear(300, 512),\n",
        "        nn.ReLU ()\n",
        "    )\n",
        "    self.linear_tahn = nn.Sequential(\n",
        "       nn.Linear(512, 512),\n",
        "       nn.ReLU ()\n",
        "    )\n",
        "    self.conv = torch.nn.Conv1d(256, 32, 2)\n",
        "  def forward(self,x):\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        x = self.linear_relu(x)\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        x = self.linear_tahn(x)\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        x = x.reshape((256, 2))\n",
        "        print(\"x.shape: \", x.shape)\n",
        "        logits = self.conv(x)\n",
        "        print(\"logits.shape: \", logits.shape)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.rand(300, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzgNL0ZAeIR2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "Ngdfr4LH_gUg",
        "outputId": "be1c067f-f72c-478b-8bc1-cb0d7c6db899"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_corpus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9bb83e8d45fa>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_corpus' is not defined"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "# Build the vocabulary\n",
        "def build_vocab(texts,tokenizer):\n",
        "    tokens = []\n",
        "    for text in texts:\n",
        "       tokens.extend(tokenizer(text))\n",
        "    return tokens\n",
        "\n",
        "train_tokens = build_vocab(train_corpus)\n",
        "vocab = build_vocab_from_iterator(train_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlFAvA0RAHim"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_paths, vocab):\n",
        "        self.data = self.load_data(file_paths)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def load_data(self, file_paths):\n",
        "        for file_path in file_paths:\n",
        "           with open(file_path, 'r', encoding='utf-8') as f:\n",
        "               tokens = tokenizer(f.read())\n",
        "        return tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        # Convert the example into numerical representations using the vocabulary\n",
        "        numerical_tokens = [self.vocab[token] for token in example]\n",
        "        return numerical_tokens\n",
        "\n",
        "\n",
        "# Create data loaders for the training and validation sets\n",
        "train_dataset = TextDataset(train_files, vocab)\n",
        "valid_dataset = TextDataset(valid_files, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzX7yBtuBu6g"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        last_hidden = output[:, -1, :]\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_52fLa0CCN1G"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "num_classes = 10\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create the model\n",
        "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkL_HdsfAYeo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "#Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Iterate over the training data for the specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    for inputs in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = torch.LongTensor(inputs)\n",
        "        targets = inputs.clone()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "        total_samples += len(inputs)\n",
        "\n",
        "    # Evaluate on the validation set after every epoch\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs in valid_loader:\n",
        "            inputs = torch.LongTensor(inputs)\n",
        "            targets = inputs.clone()\n",
        "            outputs = model(inputs)\n",
        "            val_loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n",
        "\n",
        "            total_val_loss += val_loss.item() * len(inputs)\n",
        "            total_val_samples += len(inputs)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_val_loss = total_val_loss / total_val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
