{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qivst0V8vpQe"
      },
      "source": [
        "#**Níveis de Analise Linguistica e Análise Lexical e Morfológica Automatizada - Tokenização, Stemming e Lematização**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic2dAzQo-Y9j"
      },
      "source": [
        "## Tokenização, Stemming (derivação) e Lematização\n",
        "\n",
        "### Links úteis\n",
        "\n",
        "[NLTK tutorial](https://data-flair.training/blogs/nltk-python-tutorial/)\n",
        "\n",
        "[Python stemming and lemmatization](https://data-flair.training/blogs/python-stemming/)\n",
        "\n",
        "[NLTK for portuguese](https://www.nltk.org/howto/portuguese_en.html)\n",
        "\n",
        "[Portuguese lemmatizers](https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgJDqzj_bWq"
      },
      "source": [
        "## Sumário\n",
        "- Tokenização\n",
        "- Lematização\n",
        "- Stemming\n",
        "- Níveis de análise linguística\n",
        "  - Fonética\n",
        "  - Morfológica\n",
        "  - Sintática\n",
        "  - Semântica\n",
        "  - Pragmática\n",
        "  - Texto de exemplo\n",
        "    - Árvore de dependência"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhZV9R4SxRsK"
      },
      "source": [
        "## Níveis de Análise Linguística\n",
        "\n",
        "<img src=\"https://images.slideplayer.com.br/40/11157777/slides/slide_39.jpg\" width=\"80%\">\n",
        "\n",
        "### Morfológica\n",
        "\n",
        "É o estudo da estrutura das palavras e suas classificações em diferentes categorias.\n",
        "\n",
        "Considere os exemplos:\n",
        "\n",
        "- árvore<br>\n",
        "- árvore<ins>s</ins><br>\n",
        "- arvore<ins>zinhas</ins><br>\n",
        "- <ins>im</ins>possível<br>\n",
        "- <ins>sobre</ins>mesa\n",
        "\n",
        "Essas palavras são formadas por morfemas, que se dividem em independentes e dependentes.\n",
        "\n",
        " - *árvore*, *possível*, *sobre* e *mesa* são **morfemas independentes**.\n",
        "\n",
        " - *s*, *zinhas* e *im* são **morfemas dependentes**.\n",
        "\n",
        "\n",
        "As palavras podem ser classificadas em partes do discurso (part-of-speech -> POS).\n",
        "\n",
        "***substantivos, verbos, adjetivos, preposições e advérbios*** são exemplos dessas partes do discurso.\n",
        "\n",
        "Essas classes também podem ser agrupadas entre:\n",
        " - abertas, que abrangem um grande número de palavras e abrigam facilmente novas entradas, como substantivos e verbos;\n",
        " - fechadas, que possuem funções gramaticais bem definidas, como artigos e preposições.\n",
        "\n",
        "\n",
        "\n",
        "### Sintática\n",
        "\n",
        "É o estudo do sentido entre as palavras bem como a sua disposição em uma frase.\n",
        "\n",
        "Com a classificação morfológica das palavras, uma análise sintática demonstra a validade do texto de acordo com a gramática empregada.\n",
        "\n",
        "A análise sintática também permite reconhecer os termos da oração:\n",
        "\n",
        "- Termos essenciais\n",
        " - Sujeito\n",
        " - Predicado\n",
        "\n",
        "- Termos integrantes\n",
        " - Complemento verbal\n",
        " - Complemento nominal\n",
        " - Agente da passiva\n",
        "\n",
        "- Acessórios\n",
        " - Adjunto adnominal\n",
        " - Adjunto adverbial\n",
        " - Aposto\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://linguisticageralunip.files.wordpress.com/2017/11/sintaxe-volume-1-2-638.jpg\" width=\"40%\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBjiV2-J9sie"
      },
      "source": [
        "### Exemplo prático\n",
        "Abaixo podemos ver na prática a execução das análises morfológicas e sintáticas e suas diferenças e relações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTIczJD7ItVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c7af21-c483-40db-baab-a6766cabd6fb"
      },
      "source": [
        "\n",
        "import nltk\n",
        "#download do punkt e rslp (2 pacotes que serão utilizados)\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrInbmSuTygX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8459941-be08-430b-dd22-a0d683c43381"
      },
      "source": [
        "import spacy\n",
        "\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"pt_core_news_sm\")\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ErC7VBlBSw5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "09e07287-f627-4b53-f14d-a0b177021b1d"
      },
      "source": [
        "text2 = \"O homem viu o menino com o telescópio. Ele entrou na sala de muletas.\"\n",
        "\n",
        "doc = nlp(text2)\n",
        "\n",
        "spacy.displacy.render(doc, style='dep', options={\"compact\": True, \"distance\": 100}, jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pt\" id=\"64318e2e50de4b50a1bef5b5f9e7aa91-0\" class=\"displacy\" width=\"1450\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">O</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">homem</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">viu</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">o</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">menino</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">com</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">o</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">telescópio.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">Ele</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">entrou</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">na</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">sala</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">de</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1350\">muletas.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1350\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,135.33333333333334 144.0,135.33333333333334 144.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-1\" stroke-width=\"2px\" d=\"M162,152.0 162,135.33333333333334 244.0,135.33333333333334 244.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M162,154.0 L158,146.0 166,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-2\" stroke-width=\"2px\" d=\"M362,152.0 362,135.33333333333334 444.0,135.33333333333334 444.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M362,154.0 L358,146.0 366,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-3\" stroke-width=\"2px\" d=\"M262,152.0 262,118.66666666666666 447.0,118.66666666666666 447.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M447.0,154.0 L451.0,146.0 443.0,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-4\" stroke-width=\"2px\" d=\"M562,152.0 562,118.66666666666666 747.0,118.66666666666666 747.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M562,154.0 L558,146.0 566,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-5\" stroke-width=\"2px\" d=\"M662,152.0 662,135.33333333333334 744.0,135.33333333333334 744.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M662,154.0 L658,146.0 666,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-6\" stroke-width=\"2px\" d=\"M262,152.0 262,102.0 750.0,102.0 750.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,154.0 L754.0,146.0 746.0,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-7\" stroke-width=\"2px\" d=\"M862,152.0 862,135.33333333333334 944.0,135.33333333333334 944.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M862,154.0 L858,146.0 866,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-8\" stroke-width=\"2px\" d=\"M1062,152.0 1062,135.33333333333334 1144.0,135.33333333333334 1144.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1062,154.0 L1058,146.0 1066,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-9\" stroke-width=\"2px\" d=\"M962,152.0 962,118.66666666666666 1147.0,118.66666666666666 1147.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1147.0,154.0 L1151.0,146.0 1143.0,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-10\" stroke-width=\"2px\" d=\"M1262,152.0 1262,135.33333333333334 1344.0,135.33333333333334 1344.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1262,154.0 L1258,146.0 1266,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-11\" stroke-width=\"2px\" d=\"M1162,152.0 1162,118.66666666666666 1347.0,118.66666666666666 1347.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-64318e2e50de4b50a1bef5b5f9e7aa91-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1347.0,154.0 L1351.0,146.0 1343.0,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(doc.noun_chunks))\n",
        "for token in doc:\n",
        "    print(token.morph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md7NSwwHZTiF",
        "outputId": "b8b2261c-d923-4b2e-dde8-7ab5c2743af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[O homem, o menino, o telescópio, Ele, sala, muletas]\n",
            "Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "Gender=Masc|Number=Sing\n",
            "Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            "Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "Gender=Masc|Number=Sing\n",
            "\n",
            "Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "Gender=Masc|Number=Sing\n",
            "\n",
            "Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\n",
            "Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
            "Definite=Def|Gender=Fem|Number=Sing|PronType=Art\n",
            "Gender=Fem|Number=Sing\n",
            "\n",
            "Gender=Fem|Number=Plur\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSnwdr9cyd0K"
      },
      "source": [
        "## Tokenização\n",
        "- A tokenização quebra a sequência de\n",
        "caracteres de um texto localizando o limite de cada palavra, ou seja, os pontos onde uma\n",
        "palavra termina e outra começa. As\n",
        "palavras assim identificadas são frequentemente chamadas de tokens.\n",
        "\n",
        "Vamos usar o nltk para mostrar como o texto\n",
        "\n",
        "> Dr. João foi ao banco para pegar o dinheiro que havia deixado lá. Ao chegar, decidiu se sentar para descansar um pouco.\n",
        "\n",
        "pode ser tokenizado.\n",
        "\n",
        "Para isso, primeiro é preciso baixar os pacotes *pukt* e *rslp* do NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfKno60X2Cfn"
      },
      "source": [
        "### Tokenização de sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2fUWO-VyoUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7ff1ae-df9b-47da-e57b-228f16a207a0"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text1 = \"Dr. João foi ao banco para pegar o dinheiro que havia deixado lá. Ao chegar, decidiu se sentar para descansar um pouco.\"\n",
        "\n",
        "language = \"portuguese\"\n",
        "\n",
        "sent_tokenize(text1, language)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr. João foi ao banco para pegar o dinheiro que havia deixado lá.',\n",
              " 'Ao chegar, decidiu se sentar para descansar um pouco.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zGj9l9z2MwU"
      },
      "source": [
        "### Tokenização de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pei85UWp0sKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f1492d-1abb-44e6-924c-1b987b883391"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "words = word_tokenize(text1, language)\n",
        "\n",
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dr.',\n",
              " 'João',\n",
              " 'foi',\n",
              " 'ao',\n",
              " 'banco',\n",
              " 'para',\n",
              " 'pegar',\n",
              " 'o',\n",
              " 'dinheiro',\n",
              " 'que',\n",
              " 'havia',\n",
              " 'deixado',\n",
              " 'lá',\n",
              " '.',\n",
              " 'Ao',\n",
              " 'chegar',\n",
              " ',',\n",
              " 'decidiu',\n",
              " 'se',\n",
              " 'sentar',\n",
              " 'para',\n",
              " 'descansar',\n",
              " 'um',\n",
              " 'pouco',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFALflY6EH8_"
      },
      "source": [
        "## Stemming\n",
        "O processo de stemização consiste em reduzir uma palavra ao seu radical.\n",
        "\n",
        "A palavra \"meninas\", \"meninos\" e \"menininhos\" se reduziriam a \"menin\".\n",
        "\n",
        "As palavras \"gato\", \"gata\", \"gatos\" e \"gatas\" reduziriam-se para \"gat\".\n",
        "\n",
        "A palavra pode não ter significado, daí a noção do radical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_q3S6rC4__y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089e7abd-f9ec-4a17-ea28-cf9221f54aba"
      },
      "source": [
        "nltk.download('rslp')\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "snowBallPtStemmer = nltk.stem.SnowballStemmer(\"portuguese\")\n",
        "\n",
        "for word in words:\n",
        "  print(f\"{word}: {stemmer.stem(word)} / {snowBallPtStemmer.stem(word)}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dr.: dr. / dr.\n",
            "João: joã / joã\n",
            "foi: foi / foi\n",
            "ao: ao / ao\n",
            "banco: banc / banc\n",
            "para: par / par\n",
            "pegar: peg / peg\n",
            "o: o / o\n",
            "dinheiro: dinh / dinheir\n",
            "que: que / que\n",
            "havia: hav / hav\n",
            "deixado: deix / deix\n",
            "lá: lá / lá\n",
            ".: . / .\n",
            "Ao: ao / ao\n",
            "chegar: cheg / cheg\n",
            ",: , / ,\n",
            "decidiu: decid / decid\n",
            "se: se / se\n",
            "sentar: sent / sent\n",
            "para: par / par\n",
            "descansar: descans / descans\n",
            "um: um / um\n",
            "pouco: pouc / pouc\n",
            ".: . / .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzdHUJJeFRAH"
      },
      "source": [
        "## Lematização\n",
        "\n",
        "A lematização reduz a palavra ao seu lema, que é comum entre palavras relacionadas.\n",
        "\n",
        "Para substantivo, geralmente se usa a forma no masculino e singular.\n",
        "\n",
        "No caso de verbos, o lema é o infinitivo.\n",
        "\n",
        "Por exemplo, as palavras \"gato\", \"gata\", \"gatos\" e \"gatas\" são todas formas do mesmo lema: \"gato\".\n",
        "\n",
        "Igualmente, as palavras \"tiver\", \"tenho\", \"tinha\", \"tem\" são formas do mesmo lema \"ter\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2CkdvO1E5XP"
      },
      "source": [
        "O NLTK não possui lematizador para o português, então usaremos a biblioteca Spacy.\n",
        "\n",
        "O primeiro passo é baixar e carregar o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNsU4uO6FT9-"
      },
      "source": [
        "Agora podemos extrair os lemas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBdtWkCTFU6i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f2fa56e3-0324-4fb4-fb16-4a45f0e60ff9"
      },
      "source": [
        "text = \"\"\n",
        "lemma = \"\"\n",
        "\n",
        "doc = nlp(text1)\n",
        "\n",
        "for token in nlp(text1):\n",
        "    text += token.text + \"\\t\"\n",
        "    lemma += token.lemma_ + \"\\t\"\n",
        "\n",
        "print(text)\n",
        "print(lemma)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e8eabeeb0248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuXqoJGl7vbs"
      },
      "source": [
        "A vantagem de aplicar a stemização ou lematização é clara: redução de vocabulário e abstração de significado. Porém, ao mesmo tempo, perdemos parte da informação original do texto, o que pode ser problemático para diversas aplicações, e.g., perceba a diferença de conotação entre os termos 'bom' e 'bonzinho' na descrição de um produto."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fazendo nosso próprio analisador morfológico"
      ],
      "metadata": {
        "id": "glNIFYw_n1mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!wget http://marlovss.work.gd:8080/tomorrow/aula2/bosque.conllu"
      ],
      "metadata": {
        "id": "HnkdNxnCmtn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d75269-0429-4a14-dbcd-000f79cff421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-5.0.1-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-5.0.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-5.0.1\n",
            "--2024-07-24 20:31:20--  http://marlovss.work.gd:8080/tomorrow/aula2/bosque.conllu\n",
            "Resolving marlovss.work.gd (marlovss.work.gd)... 177.180.149.154\n",
            "Connecting to marlovss.work.gd (marlovss.work.gd)|177.180.149.154|:8080... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11291250 (11M)\n",
            "Saving to: ‘bosque.conllu’\n",
            "\n",
            "bosque.conllu       100%[===================>]  10.77M  1.13MB/s    in 14s     \n",
            "\n",
            "2024-07-24 20:31:35 (802 KB/s) - ‘bosque.conllu’ saved [11291250/11291250]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conllu\n",
        "import itertools as it\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "class CoNLLU:\n",
        "   def __init__(self, files):\n",
        "      self.word = []\n",
        "      self.sentences = []\n",
        "      for f in files:\n",
        "         parsed = conllu.parse(open(f).read())\n",
        "         sents = [[AttributeDict(form = token['form'], lemma=token['lemma'],pos=token['upos'],feats=token['feats']) for token in tokenlist if token['upos']!='_'] for tokenlist in parsed]\n",
        "         self.sentences.extend(sents)\n",
        "         self.words.extend([word for sent in sents for word in sent])\n",
        "      self.pos_tags = set([word.pos for word in self.words])\n",
        "      self.feats_dict ={pos:set(it.chain.from_iterable([list(word.feats.keys()) for word in self.words if word.pos==pos and word.feats!= None])) for pos in self.pos_tags}\n"
      ],
      "metadata": {
        "id": "6whsxfkKmTnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bosque = CoNLLU(files=[\"bosque.conllu\"])"
      ],
      "metadata": {
        "id": "JY5mcN4Em2zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bosque.feats_dict"
      ],
      "metadata": {
        "id": "7ITYvRLbm4cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd4f53c-91a9-41b9-dc6c-ee6e85d89f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PART': {'ExtPos', 'Gender', 'Number'},\n",
              " 'NUM': {'ExtPos', 'Gender', 'NumType', 'Number'},\n",
              " 'SYM': set(),\n",
              " 'CCONJ': {'ExtPos', 'Number'},\n",
              " 'PRON': {'Case',\n",
              "  'Definite',\n",
              "  'ExtPos',\n",
              "  'Gender',\n",
              "  'Number',\n",
              "  'Person',\n",
              "  'PronType',\n",
              "  'Reflex',\n",
              "  'Typo'},\n",
              " 'SCONJ': {'Definite',\n",
              "  'ExtPos',\n",
              "  'Gender',\n",
              "  'Number',\n",
              "  'PronType',\n",
              "  'Typo',\n",
              "  'VerbForm'},\n",
              " 'DET': {'Definite',\n",
              "  'ExtPos',\n",
              "  'Gender',\n",
              "  'NumType',\n",
              "  'Number',\n",
              "  'Poss',\n",
              "  'PronType',\n",
              "  'Typo'},\n",
              " 'ADV': {'ExtPos', 'Gender', 'Number', 'Polarity', 'PronType', 'Typo'},\n",
              " 'X': {'ExtPos', 'Gender', 'Number'},\n",
              " 'INTJ': set(),\n",
              " 'AUX': {'ExtPos', 'Gender', 'Mood', 'Number', 'Person', 'Tense', 'VerbForm'},\n",
              " 'VERB': {'ExtPos',\n",
              "  'Gender',\n",
              "  'Mood',\n",
              "  'Number',\n",
              "  'Person',\n",
              "  'Tense',\n",
              "  'Typo',\n",
              "  'VerbForm',\n",
              "  'Voice'},\n",
              " 'ADJ': {'Abbr',\n",
              "  'Degree',\n",
              "  'ExtPos',\n",
              "  'Gender',\n",
              "  'NumType',\n",
              "  'Number',\n",
              "  'PronType',\n",
              "  'Typo',\n",
              "  'Voice'},\n",
              " 'PROPN': {'Abbr', 'ExtPos', 'Gender', 'Number', 'PronType', 'Typo'},\n",
              " 'ADP': {'ExtPos', 'Gender', 'Number'},\n",
              " 'PUNCT': set(),\n",
              " 'NOUN': {'Abbr',\n",
              "  'ExtPos',\n",
              "  'Foreign',\n",
              "  'Gender',\n",
              "  'NumType',\n",
              "  'Number',\n",
              "  'Typo',\n",
              "  'Voice'}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vamos começar com um etiquetador morfossintático (POS tagger).\n",
        "\n",
        "Um etiquetador morfossintático tem por objetivo identificar a classe gramatical das palavras em uma sentença, ou conjunto de sentenças. Por exemplo, considere a frase:\n",
        "\n",
        "O rato roeu a roupa do rei de Roma.\n",
        "\n",
        "Após tokenizada, temos:\n",
        "O, rato, roeu, a, roupa, de, o, rei, de, Roma\n",
        "\n",
        "O resultado do etiquetador sobre esse conjunto é:\n",
        "(O, DET), (rato, NOUN), (roeu, VERB), (a, DET), (roupa, NOUN), (de, ADP), (o,DET), (rei, NOUN), (de, ADP), (Roma, PROPN)\n",
        "\n",
        "O conjunto de etiquetas (tagset) que descrevem as classes gramaticais é determinado no projeto do corpus e **não constituem um conjunto universal ou objetivo**, mas uma decisão teórico-metodológica do projeto.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "s8coMQj3nyJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(predicted,gold):\n",
        "   acertos = len([predicted[i][j][1] for i in range(len(gold)) for j in range(len(gold[i])) if predicted[i][j][1]==gold[i][j][1]])\n",
        "   totais = sum([len(sent) for sent in gold])\n",
        "   return acertos/totais\n"
      ],
      "metadata": {
        "id": "LBxsQLblfjpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "ADP = [\"de\",\"em\",\"para\",\"com\",\"sobre\",\"sob\"]\n",
        "PRON = [\"ele\",\"ela\",\"eles\",\"elas\",\"eu\",\"nós\",\"tu\",\"vós\",\"você\",\"vc\",\"vocês\"]\n",
        "suffixes = set([word.form.lower()[-3:] for word in bosque.words])\n",
        "suf_to_tag = {suf:FreqDist([word.pos for word in bosque.words if word.form.lower()[-3:]==suf]).max() for suf in suffixes}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Hm33XPLszLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(suf_to_tag)"
      ],
      "metadata": {
        "id": "4fF8hIaviS_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tag(tokens):\n",
        "  tagged = []\n",
        "  for token in tokens:\n",
        "    if token.lower() in ADP:\n",
        "       tagged.append((token,\"ADP\"))\n",
        "    elif token.lower() in PRON:\n",
        "       tagged.append((token,\"PRON\"))\n",
        "    else:\n",
        "       tagged.append((token,\"_\"))\n",
        "  return tagged"
      ],
      "metadata": {
        "id": "Z_JN8UjQiQV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://marlovss.work.gd:8080/tomorrow/aula2/test.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xsx4167f0Az",
        "outputId": "e9bde269-add2-4a74-b80d-0acb2b41916d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-24 20:47:36--  http://marlovss.work.gd:8080/tomorrow/aula2/test.conllu\n",
            "Resolving marlovss.work.gd (marlovss.work.gd)... 177.180.149.154\n",
            "Connecting to marlovss.work.gd (marlovss.work.gd)|177.180.149.154|:8080... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1819980 (1.7M)\n",
            "Saving to: ‘test.conllu’\n",
            "\n",
            "test.conllu         100%[===================>]   1.74M   522KB/s    in 3.4s    \n",
            "\n",
            "2024-07-24 20:47:41 (522 KB/s) - ‘test.conllu’ saved [1819980/1819980]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = CoNLLU(files=[\"test.conllu\"])\n",
        "test_sents = [[word.form for word in sent] for sent in test.sentences]\n",
        "gold = [[(word.form.lower(),word.pos) for word in sent] for sent in test.sentences]\n",
        "predicted = [tag(sent) for sent in test_sents]\n",
        "\n",
        "accuracy(predicted,gold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43XolYbwfs9q",
        "outputId": "52313b64-d7eb-45f5-f618-61393ca90ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1203086509201565"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fazendo nosso lematizador e analisador de flexões\n",
        "\n",
        "O lematizador e a análise flexional podem ser realizados em conuunto, uma vez que determinar as flexões nos permitem \"desfazê-las\", i.e. obter uma versão \"normalizada\" do item lexical"
      ],
      "metadata": {
        "id": "AXIDA2Tis3Al"
      }
    }
  ]
}