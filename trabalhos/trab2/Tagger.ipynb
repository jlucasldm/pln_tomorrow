{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgV2PmKMGQZ-",
        "outputId": "2bb73d24-df17-43c0-e801-e3b1dc1010ea"
      },
      "outputs": [],
      "source": [
        "# !pip install conllu\n",
        "# !wget http://marlovss.work.gd:8080/tomorrow/aula2/bosque.conllu\n",
        "\n",
        "\"\"\"\n",
        "Implemente um classificador morfossintático através da estratégia de classificação de palavras. \n",
        "Você pode utilizar para tal implementação a classe ClassifierBasedTagger do NLTK, definindo uma \n",
        "função de representação (extração de features) e um classificador qualquer da API nltk.classify.\n",
        "\"\"\"\n",
        "\n",
        "import conllu\n",
        "import itertools as it\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "class CoNLLU:\n",
        "   def __init__(self, files):\n",
        "      self.words = []\n",
        "      self.sentences = []\n",
        "      for f in files:\n",
        "         parsed = conllu.parse(open(f, encoding=\"utf8\").read())\n",
        "         sents = [[AttributeDict(form = token['form'], lemma=token['lemma'],pos=token['upos'],feats=token['feats']) for token in tokenlist if token['upos']!='_'] for tokenlist in parsed]\n",
        "         self.sentences.extend(sents)\n",
        "         self.words.extend([word for sent in sents for word in sent])\n",
        "      self.pos_tags = set([word.pos for word in self.words])\n",
        "      self.feats_dict ={pos:set(it.chain.from_iterable([list(word.feats.keys()) for word in self.words if word.pos==pos and word.feats!= None])) for pos in self.pos_tags}\n",
        "\n",
        "bosque = CoNLLU(files=[\"bosque.conllu\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUaxlIs4Fw_Y"
      },
      "source": [
        "## Exemplo de função de representação.\n",
        "**NOTE**: As funções de representação devem sempre receber três argumentos: uma lista de tokens, o índice do token sendo processado e um histórico das classificações já realizadas pelo tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "NZL9yqNWHh8V"
      },
      "outputs": [],
      "source": [
        "# # Draft\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   # feats['token_lower'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-5:]\n",
        "#   # feats['suffix'] = tokens[index][0].lower()[len(stem):]\n",
        "#   # feats['stem'] = stem\n",
        "\n",
        "#   # feats['token_position'] = index\n",
        "  \n",
        "#   # feats['suffix1'] = tokens[index][0].lower()[-1:]\n",
        "#   # feats['suffix2'] = tokens[index][0].lower()[-2:]\n",
        "#   # feats['suffix3'] = tokens[index][0].lower()[-3:]\n",
        "#   # feats['suffix4'] = tokens[index][0].lower()[-4:]\n",
        "#   # feats['prefix1'] = tokens[index][0].lower()[:1]\n",
        "#   # feats['prefix2'] = tokens[index][0].lower()[:2]\n",
        "#   # feats['prefix3'] = tokens[index][0].lower()[:3]\n",
        "#   # feats['prefix4'] = tokens[index][0].lower()[:4]\n",
        "\n",
        "#   # feats['token_before1'] = tokens[index-1][0] if index > 0 else '<start>'\n",
        "#   # feats['suf_before1'] = tokens[index-1][0].lower()[-4:] if index > 0 else '<start>'\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "\n",
        "#   # feats['token_before2'] = tokens[index-2][0] if index > 1 else '<start>'\n",
        "#   # feats['suf_before2'] = tokens[index-2][0].lower()[-4:] if index > 1 else '<start>'\n",
        "#   feats['pos_before2'] = history[index-2][1] if index > 1 else '<start>'\n",
        "\n",
        "#   # feats['token_before3'] = tokens[index-3][0] if index > 2 else '<start>'\n",
        "#   # feats['suf_before3'] = tokens[index-3][0].lower()[-4:] if index > 2 else '<start>'\n",
        "#   # feats['pos_before3'] = history[index-3][1] if index > 2 else '<start>'\n",
        "\n",
        "#   # feats['token_after'] = tokens[index+1][0] if index < len(tokens)-1 else '<end>'\n",
        "#   feats['suf_after'] = tokens[index+1][0].lower()[-5:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "#   # feats['token_after2'] = tokens[index+2][0] if index < len(tokens)-2 else '<end>'\n",
        "#   # feats['suf_after2'] = tokens[index+2][0].lower()[-4:] if index < len(tokens)-2 else '<end>'\n",
        "  \n",
        "#   # feats['token_after3'] = tokens[index+3][0] if index < len(tokens)-3 else '<end>'\n",
        "#   # feats['suf_after3'] = tokens[index+3][0].lower()[-4:] if index < len(tokens)-3 else '<end>'\n",
        "  \n",
        "#   # feats['is_upper'] = tokens[index][0].isupper()\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 0.4419243666169896 \n",
        "# # using decision tree\n",
        "\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-4:]\n",
        "\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 0.4176485655737705\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-4:]\n",
        "\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "\n",
        "#   feats['suf_after'] = tokens[index+1][0].lower()[-4:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 0.36805490871833085\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-5:]\n",
        "\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "#   feats['pos_before2'] = history[index-2][1] if index > 1 else '<start>'\n",
        "\n",
        "#   feats['suf_after'] = tokens[index+1][0].lower()[-5:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "\n",
        "def repr(tokens,index,history):\n",
        "  feats ={}\n",
        "\n",
        "  stem = stemmer.stem(tokens[index][0].lower())\n",
        "  \n",
        "  feats['token'] = tokens[index][0].lower()\n",
        "  feats['suffix'] = tokens[index][0].lower()[len(stem):]\n",
        "  feats['token_length'] = len(tokens[index][0])\n",
        "\n",
        "  # feats['suffix1'] = tokens[index][0].lower()[-1:]\n",
        "  # feats['suffix2'] = tokens[index][0].lower()[-2:]\n",
        "  # feats['suffix3'] = tokens[index][0].lower()[-3:]\n",
        "\n",
        "  feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "  feats['pos_before2'] = history[index-2][1] if index > 1 else '<start>'\n",
        "\n",
        "  # feats['suf_after3'] = tokens[index+1][0].lower()[-3:] if index < len(tokens)-1 else '<end>'\n",
        "  # feats['suf_after2'] = tokens[index+1][0].lower()[-2:] if index < len(tokens)-1 else '<end>'\n",
        "  # feats['suf_after1'] = tokens[index+1][0].lower()[-1:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "  feats['suffix_after'] = tokens[index+1][0].lower()[len(stemmer.stem(tokens[index+1][0].lower())):] if index < len(tokens)-1 else '<end>'\n",
        "  feats['token_after'] = tokens[index+1][0].lower() if index < len(tokens)-1 else '<end>'\n",
        "  feats['token_length_after'] = len(tokens[index+1][0]) if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "  # feats['suf_after'] = tokens[index+1][0].lower()[-5:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "  # feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "  # feats['is_digit'] = tokens[index][0].isdigit()\n",
        "  # feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "  return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "EKyH5J9UFLUD"
      },
      "outputs": [],
      "source": [
        "from nltk.tag.sequential import ClassifierBasedTagger\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "tagged_sents = [[(word.form,word.pos) for word in sent] for sent in bosque.sentences]\n",
        "data = [(repr(sent, i, sent[:i]), sent[i][1]) for sent in tagged_sents for i in range(len(sent))]\n",
        "# labels = [sent[i][1] for sent in tagged_sents for i in range(len(sent))]\n",
        "# features = [list(repr(sent, i, sent[:i]).values()) for sent in tagged_sents for i in range(len(sent))]\n",
        "\n",
        "# data = [(list(repr(sent, i, sent[:i]).values()), sent[i][1]) for sent in tagged_sents for i in range(len(sent))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Convertendo para arrays NumPy\n",
        "# features_np = np.array(features)\n",
        "# labels_np = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "C9F1F6dzECcJ"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[177], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Utilizando o classificador DecisionTree\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# classifier = SklearnClassifier(DecisionTreeClassifier(criterion='entropy'))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# classifier = SklearnClassifier(KNeighborsClassifier(n_neighbors=3))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# classifier = classifier.train(data)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# tagger = ClassifierBasedTagger(feature_detector=repr, classifier = classifier)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m tagger \u001b[38;5;241m=\u001b[39m \u001b[43mClassifierBasedTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_detector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNaiveBayesClassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tag\\sequential.py:643\u001b[0m, in \u001b[0;36mClassifierBasedTagger.__init__\u001b[1;34m(self, feature_detector, train, classifier_builder, classifier, backoff, cutoff_prob, verbose)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The classifier used to choose a tag for each token.\"\"\"\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\nltk\\tag\\sequential.py:671\u001b[0m, in \u001b[0;36mClassifierBasedTagger._train\u001b[1;34m(self, tagged_corpus, classifier_builder, verbose)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tagged_corpus:\n\u001b[0;32m    670\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 671\u001b[0m     untagged_sentence, tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msentence)\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[0;32m    673\u001b[0m         featureset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_detector(untagged_sentence, index, history)\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "# Utilizando o classificador DecisionTree\n",
        "# classifier = SklearnClassifier(DecisionTreeClassifier(criterion='entropy'))\n",
        "# classifier = SklearnClassifier(KNeighborsClassifier(n_neighbors=3))\n",
        "# classifier = SklearnClassifier(SVC(kernel='linear', C=1))\n",
        "\n",
        "classifier = SklearnClassifier(NaiveBayesClassifier())\n",
        "\n",
        "classifier = classifier.train(data)\n",
        "tagger = ClassifierBasedTagger(feature_detector=repr, classifier = classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goCyj2czAuzs",
        "outputId": "adfa18e2-614b-438a-afa5-68b64641d8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('o', 'DET'), ('rato', 'PROPN'), ('roeu', 'PROPN'), ('a', 'ADP'), ('roupa', 'PROPN'), ('do', 'PUNCT'), ('rei', 'PROPN'), ('de', 'PUNCT'), ('roma', 'PUNCT')]\n",
            "0.3224082526080477\n"
          ]
        }
      ],
      "source": [
        "print(tagger.tag([\"o\",\"rato\",\"roeu\",\"a\",\"roupa\",\"do\",\"rei\",\"de\",\"roma\"]))\n",
        "print(tagger.accuracy(tagged_sents))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
