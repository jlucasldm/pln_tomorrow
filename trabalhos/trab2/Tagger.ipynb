{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgV2PmKMGQZ-",
        "outputId": "2bb73d24-df17-43c0-e801-e3b1dc1010ea"
      },
      "outputs": [],
      "source": [
        "# !pip install conllu\n",
        "# !wget http://marlovss.work.gd:8080/tomorrow/aula2/bosque.conllu\n",
        "\n",
        "\"\"\"\n",
        "Implemente um classificador morfossintático através da estratégia de classificação de palavras. \n",
        "Você pode utilizar para tal implementação a classe ClassifierBasedTagger do NLTK, definindo uma \n",
        "função de representação (extração de features) e um classificador qualquer da API nltk.classify.\n",
        "\"\"\"\n",
        "\n",
        "import conllu\n",
        "import itertools as it\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "class CoNLLU:\n",
        "   def __init__(self, files):\n",
        "      self.words = []\n",
        "      self.sentences = []\n",
        "      for f in files:\n",
        "         parsed = conllu.parse(open(f, encoding=\"utf8\").read())\n",
        "         sents = [[AttributeDict(form = token['form'], lemma=token['lemma'],pos=token['upos'],feats=token['feats']) for token in tokenlist if token['upos']!='_'] for tokenlist in parsed]\n",
        "         self.sentences.extend(sents)\n",
        "         self.words.extend([word for sent in sents for word in sent])\n",
        "      self.pos_tags = set([word.pos for word in self.words])\n",
        "      self.feats_dict ={pos:set(it.chain.from_iterable([list(word.feats.keys()) for word in self.words if word.pos==pos and word.feats!= None])) for pos in self.pos_tags}\n",
        "\n",
        "bosque = CoNLLU(files=[\"bosque.conllu\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUaxlIs4Fw_Y"
      },
      "source": [
        "## Exemplo de função de representação.\n",
        "**NOTE**: As funções de representação devem sempre receber três argumentos: uma lista de tokens, o índice do token sendo processado e um histórico das classificações já realizadas pelo tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "NZL9yqNWHh8V"
      },
      "outputs": [],
      "source": [
        "# # Draft\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   # feats['token_lower'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-5:]\n",
        "#   # feats['suffix'] = tokens[index][0].lower()[len(stem):]\n",
        "#   # feats['stem'] = stem\n",
        "\n",
        "#   # feats['token_position'] = index\n",
        "  \n",
        "#   # feats['suffix1'] = tokens[index][0].lower()[-1:]\n",
        "#   # feats['suffix2'] = tokens[index][0].lower()[-2:]\n",
        "#   # feats['suffix3'] = tokens[index][0].lower()[-3:]\n",
        "#   # feats['suffix4'] = tokens[index][0].lower()[-4:]\n",
        "#   # feats['prefix1'] = tokens[index][0].lower()[:1]\n",
        "#   # feats['prefix2'] = tokens[index][0].lower()[:2]\n",
        "#   # feats['prefix3'] = tokens[index][0].lower()[:3]\n",
        "#   # feats['prefix4'] = tokens[index][0].lower()[:4]\n",
        "\n",
        "#   # feats['token_before1'] = tokens[index-1][0] if index > 0 else '<start>'\n",
        "#   # feats['suf_before1'] = tokens[index-1][0].lower()[-4:] if index > 0 else '<start>'\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "\n",
        "#   # feats['token_before2'] = tokens[index-2][0] if index > 1 else '<start>'\n",
        "#   # feats['suf_before2'] = tokens[index-2][0].lower()[-4:] if index > 1 else '<start>'\n",
        "#   feats['pos_before2'] = history[index-2][1] if index > 1 else '<start>'\n",
        "\n",
        "#   # feats['token_before3'] = tokens[index-3][0] if index > 2 else '<start>'\n",
        "#   # feats['suf_before3'] = tokens[index-3][0].lower()[-4:] if index > 2 else '<start>'\n",
        "#   # feats['pos_before3'] = history[index-3][1] if index > 2 else '<start>'\n",
        "\n",
        "#   # feats['token_after'] = tokens[index+1][0] if index < len(tokens)-1 else '<end>'\n",
        "#   feats['suf_after'] = tokens[index+1][0].lower()[-5:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "#   # feats['token_after2'] = tokens[index+2][0] if index < len(tokens)-2 else '<end>'\n",
        "#   # feats['suf_after2'] = tokens[index+2][0].lower()[-4:] if index < len(tokens)-2 else '<end>'\n",
        "  \n",
        "#   # feats['token_after3'] = tokens[index+3][0] if index < len(tokens)-3 else '<end>'\n",
        "#   # feats['suf_after3'] = tokens[index+3][0].lower()[-4:] if index < len(tokens)-3 else '<end>'\n",
        "  \n",
        "#   # feats['is_upper'] = tokens[index][0].isupper()\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 0.4419243666169896 \n",
        "# # using decision tree\n",
        "\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-4:]\n",
        "\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 0.4176485655737705\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-4:]\n",
        "\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "\n",
        "#   feats['suf_after'] = tokens[index+1][0].lower()[-4:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 0.36805490871833085\n",
        "# def repr(tokens,index,history):\n",
        "#   feats ={}\n",
        "  \n",
        "#   feats['token'] = tokens[index][0].lower()\n",
        "#   feats['suffix'] = tokens[index][0].lower()[-5:]\n",
        "\n",
        "#   feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "#   feats['pos_before2'] = history[index-2][1] if index > 1 else '<start>'\n",
        "\n",
        "#   feats['suf_after'] = tokens[index+1][0].lower()[-5:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "#   feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "#   feats['is_digit'] = tokens[index][0].isdigit()\n",
        "#   feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "#   return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "\n",
        "def repr(tokens,index,history):\n",
        "  feats ={}\n",
        "\n",
        "  stem = stemmer.stem(tokens[index][0].lower())\n",
        "  \n",
        "  feats['token'] = tokens[index][0].lower()\n",
        "  feats['suffix'] = tokens[index][0].lower()[len(stem):]\n",
        "  feats['token_length'] = len(tokens[index][0])\n",
        "\n",
        "  # feats['suffix1'] = tokens[index][0].lower()[-1:]\n",
        "  # feats['suffix2'] = tokens[index][0].lower()[-2:]\n",
        "  # feats['suffix3'] = tokens[index][0].lower()[-3:]\n",
        "\n",
        "  feats['pos_before1'] = history[index-1][1] if index > 0 else '<start>'\n",
        "  feats['pos_before2'] = history[index-2][1] if index > 1 else '<start>'\n",
        "\n",
        "  # feats['suf_after3'] = tokens[index+1][0].lower()[-3:] if index < len(tokens)-1 else '<end>'\n",
        "  # feats['suf_after2'] = tokens[index+1][0].lower()[-2:] if index < len(tokens)-1 else '<end>'\n",
        "  # feats['suf_after1'] = tokens[index+1][0].lower()[-1:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "  feats['suffix_after'] = tokens[index+1][0].lower()[len(stemmer.stem(tokens[index+1][0].lower())):] if index < len(tokens)-1 else '<end>'\n",
        "  feats['token_after'] = tokens[index+1][0].lower() if index < len(tokens)-1 else '<end>'\n",
        "  feats['token_length_after'] = len(tokens[index+1][0]) if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "  # feats['suf_after'] = tokens[index+1][0].lower()[-5:] if index < len(tokens)-1 else '<end>'\n",
        "\n",
        "  # feats['is_punct'] = tokens[index][0] in ['.',',','!','?',';',':','(',')','[',']','{','}','\"','`',\"'\",'-','_','/','\\\\','|','@','#','$','%','^','&','*','+','=','<','>','~']\n",
        "  # feats['is_digit'] = tokens[index][0].isdigit()\n",
        "  # feats['is_adp'] = tokens[index][0] in [\"ante\", \"após\", \"até\", \"com\", \"contra\", \"de\", \"desde\", \"em\", \"entre\", \"para\", \"perante\", \"por\", \"sem\", \"sob\", \"sobre\", \"trás\"]\n",
        "\n",
        "  return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "EKyH5J9UFLUD"
      },
      "outputs": [],
      "source": [
        "from nltk.tag.sequential import ClassifierBasedTagger\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "\n",
        "tagged_sents = [[(word.form,word.pos) for word in sent] for sent in bosque.sentences]\n",
        "# data = [(repr(sent, i, sent[:i]), sent[i][1]) for sent in tagged_sents for i in range(len(sent))]\n",
        "# labels = [sent[i][1] for sent in tagged_sents for i in range(len(sent))]\n",
        "# features = [list(repr(sent, i, sent[:i]).values()) for sent in tagged_sents for i in range(len(sent))]\n",
        "\n",
        "train_X = [list(repr(sent, i, sent[:i]).values()) for sent in tagged_sents for i in range(len(sent))]\n",
        "train_y = [sent[i][1] for sent in tagged_sents for i in range(len(sent))]\n",
        "\n",
        "# data = [(list(repr(sent, i, sent[:i]).values()), sent[i][1]) for sent in tagged_sents for i in range(len(sent))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_np = np.array(train_X)\n",
        "train_y_np = np.array(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[184], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[1;32m----> 2\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_np\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:267\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    266\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_refit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\naive_bayes.py:428\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[1;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m first_call \u001b[38;5;241m=\u001b[39m _check_partial_fit_first_call(\u001b[38;5;28mself\u001b[39m, classes)\n\u001b[1;32m--> 428\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:910\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead."
          ]
        }
      ],
      "source": [
        "clf = GaussianNB()\n",
        "clf = clf.fit(train_x_np, train_y_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9F1F6dzECcJ"
      },
      "outputs": [],
      "source": [
        "# Utilizando o classificador DecisionTree\n",
        "# classifier = SklearnClassifier(DecisionTreeClassifier(criterion='entropy'))\n",
        "# classifier = SklearnClassifier(KNeighborsClassifier(n_neighbors=3))\n",
        "# classifier = SklearnClassifier(SVC(kernel='linear', C=1))\n",
        "\n",
        "# classifier = classifier.train(data)\n",
        "# tagger = ClassifierBasedTagger(feature_detector=repr, classifier = classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goCyj2czAuzs",
        "outputId": "adfa18e2-614b-438a-afa5-68b64641d8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('o', 'DET'), ('rato', 'PROPN'), ('roeu', 'PROPN'), ('a', 'ADP'), ('roupa', 'PROPN'), ('do', 'PUNCT'), ('rei', 'PROPN'), ('de', 'PUNCT'), ('roma', 'PUNCT')]\n",
            "0.3224082526080477\n"
          ]
        }
      ],
      "source": [
        "# print(tagger.tag([\"o\",\"rato\",\"roeu\",\"a\",\"roupa\",\"do\",\"rei\",\"de\",\"roma\"]))\n",
        "# print(tagger.accuracy(tagged_sents))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
